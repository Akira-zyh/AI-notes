# 马尔可夫决策过程

- 智能体（agent）：**强化学习的主体**
- 环境（environment）：与智能体交互的对象（<font color="#e36c09">交互过程中的规则或机理</font>）
	- <span style="background:#fff88f">状态（state）</span>：**对当前环境的概括**，是<font color="#e36c09">决策的依据</font>
		- <span style="background:#fff88f">状态空间（state space）</span>：**所有可能状态的集合**$\textcolor{orange}{\mathcal{S}}$（有限集合 or 无限集合）
	- <span style="background:#d3f8b6">动作（action）</span>：**基于当前状态**做出的<font color="#e36c09">决策</font>
		- <span style="background:#d3f8b6">动作空间（action space）</span>：**所有可能动作的集合**$\textcolor{orange}{\mathcal{A}}$
	- <span style="background:#b1ffff">奖励（reward）</span>：智能体执行一个*动作*后，<font color="#e36c09">环境返回给智能体的一个数值</font>
	- <span style="background:#fdbfff">状态转移（state transition）</span>：智能体**从当前 $\textcolor{orange}t$ 时刻状态 $\textcolor{orange}s$ 转移到下一个时刻状态 $\textcolor{orange}{s'}$ 的过程**
		- <span style="background:#fdbfff">状态转移概率函数（state transition probability function）</span>：记作$\textcolor{orange}{p_t(s'|s,a)\text{或}\tau(s,a)}$表示事件的概率$$\textcolor{orange}{p_t(s' | s, a)=\mathbb{P}(S'_{t+1}=s'|S_t=s, A_t=a)}$$

---
## 策略

- 策略（policy）：
	- 随机策略（random policy）：
	- 确定策略（certain policy）：

> [!info] 智能体与环境交互（agent environment interaction）
> 

- 回合（episodes）：

---
## 随机性来源

> [!warning] 随机性的来源
> 动作的随机性
>>随机决策
>
>状态的随机性
>> 状态转移函数

- 马尔可夫性质（Markov property）：
- 轨迹（trajectory）：

---

## 回报与折扣回报


> [!NOTE] 回报（reward）

> 回报（return）也叫做累积奖励（cumulative future reward）

> [!NOTE] 折扣回报（discount reward）
> 


> [!warning] 回报$U_t$的随机性来源
> 未观测到的动作和状态：$$A_t\text{, }\;S_{t+1}\text{,}A_{t+1}\text{, }\;\dots S_n,A_n$$

## 价值函数

### 动作价值函数（action-value function）
$$\textcolor{orange}{Q_{\pi}(s_t, a_t)=\mathbb{E}_{S_{t+1}\text{,}A_{t+1}\text{, }\dots\text{,} S_n,A_n}[U_t|S_t=s_t, A_t = a_t]}$$

#### 最优动作价值函数

$$\textcolor{orange}{Q_{\star}(s_t,a_t)=\max_{\pi}{Q_{\pi}(s_t, a_t)}}, \;\forall s_t\in \mathcal{S},\; \forall a_t \in \mathcal{A}$$
$$\textcolor{orange}{\pi_{\star}=\mathop{\arg\max}\limits_{\pi}\,{Q_{\pi}(s_t, a_t)}}, \;\forall s_t\in \mathcal{S},\; \forall a_t \in \mathcal{A}$$
---

### 状态价值函数（state-value function）
1. 
$$\textcolor{orange}{V_{\pi}(s_t)=\mathbb{E}_{A_t \sim \pi(\cdot|s_t)}\big[Q_{\pi}(s_t, A_t)\big]}$$
$$\textcolor{orange}{=\sum_{a\in \mathcal{A}}\pi(a|s_t) \cdot Q_{\pi}(s_t, a)}$$
2. 
$$\textcolor{orange}{V_{\pi}(s_t) = \mathbb{E}_{S_{t+1}\text{,}A_{t+1}\text{, }\dots\text{,} S_n,A_n}\big[U_t\big|S_t=a_t\big]}$$
